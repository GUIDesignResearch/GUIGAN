# GUIGAN: Learning to Generate GUI Designs Using Generative Adversarial Networks

Graphical User Interface (GUI) is ubiquitous in almost all modern desktop software, mobile applications and online websites. A good GUI design is crucial to the success of the
software in the market, but designing a good GUI which requires much innovation and creativity is difficult even to well-trained designers. In addition, the requirement of rapid development of GUI design also aggravates designers’ working load. So, the availability of various automated generated GUIs can help enhance the design personalization and specialization as they can cater to the taste of different designers. 

To assist designers, we develop a model GUIGAN to automatically generate GUI designs. Different from conventional image generation models based on image pixels, our GUIGAN is to reuse GUI components collected from existing mobile app GUIs for composing a new design which is similar to natural-language generation. Our GUIGAN is based on SeqGAN by modelling the GUI component style compatibility and GUI structure. The evaluation demonstrates that our model significantly outperforms the best of the baseline methods by 30.77% in Frechet Inception distance (FID) and 12.35% in 1-Nearest Neighbor Accuracy (1-NNA). Through a pilot user study, we provide initial evidence of the usefulness of our approach for generating acceptable brand new GUI designs. We formulate our task as generating a new GUI by selecting a list of compatible GUI subtrees.


## Task Establishment
One GUI design image consists of two types of components i.e., widgets (e.g., button, image, text) and spatial layouts (e.g., linear layout, relative layout). The widgets (leaf nodes) are organized by the layout (intermedia nodes) as the structural tree for one GUI design. We take the subtree of existing GUIs as the basic unit for composing a new GUI design rather than plain pixels. 

We cut these candidate subtrees from the original screenshot according to certain rules. Given one GUI design with detailed component information, we cut out all the first-level subtrees from the original DOM tree . If the width of a subtree exceeds 90% of the GUI width, we continue to cut it to the next level, otherwise we stop splitting and this subtree is used as the smallest granularity unit. The procedure will be iterated until all the segmentation stops. Finally, we use all the smallest subtrees as indexes to identify templates. 

We remove the subtrees with duplicate bounds in one GUI and keep only one in the process. Besides, subtrees with partial overlap and too high or too low aspect ratio are also removed, which can not be cut from the original GUI.

![Alt text](https://github.com/GUIDesignResearch/GUIGAN/blob/master/Display/Fig2_cut.jpg)

The figure shows an example segmentation of a real GUI screen shot, and each subtree is used as the basic unit in our work. 


##  Learning to Generate GUI Designs Using Generative Adversarial Networks

![Alt text](https://github.com/GUIDesignResearch/GUIGAN/blob/master/Display/Fig1_v2.jpg)

An overview of our approach is shown in the figure. First, We collect 12,230 GUI screenshots and their corresponding metainformation from 1,609 Android apps in 27 categories from Google Play and decompose them into 41,813 component subtrees for re-using. Second, we develop a SeqGAN based model. Apart from the default generation and discrimination loss, we model the GUI component style compatibility and GUI layout structure for guiding the training. Therefore, our GUIGAN can generate brand new GUI designs for designers’ inspiration. 


### Style Embedding of Subtree

### Modeling Subtree Compatibility

### Modeling Subtree Structure

### Multi-Loss Fusion


## IMPLEMENTATION
Our data comes from Rico(from **[`Rico dataset`](http://interactionmining.org/rico)**), an open source mobile app dataset for building data-driven design applications. 

Through manual selection, we have collected relatively professional and more suitable apps for this study. The apps with more images, animation or game screens are not selected. In addition, the GUIs with large pop-up areas, Web links waiting, and full screen ads are not selected.

We try to test our model’s capability in capturing that characteristic by preparing separated dataset for five most frequent app categories in Rico dataset, including News & Magazines, Books & Reference, Shopping, Communication, and Travel & Local. 


## Automated Evaluation


**Samples generated by GUIGAN.**

![Alt text](https://github.com/GUIDesignResearch/GUIGAN/blob/master/Display/Display.jpg)


## HUMAN EVALUATION


